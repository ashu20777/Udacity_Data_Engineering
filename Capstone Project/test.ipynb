{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('C:\\spark')\n",
    "               \n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "#df_visits = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "#df_visits.write.parquet(\"sas_data\")\n",
    "df_visits=spark.read.parquet(\"data/immigration_data\")\n",
    "df_demo = spark.read.csv(\"data/us-cities-demographics.csv\", sep=\";\", header=True)\n",
    "df_airports = spark.read.csv(\"data/airport-codes_csv.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing cleaning tasks here\n",
    "# df_visits.show(3)\n",
    "#df_visits.describe()\n",
    "df_visits.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|country_code|        country_name|\n",
      "+------------+--------------------+\n",
      "|         582|MEXICO Air Sea, a...|\n",
      "|         236|         AFGHANISTAN|\n",
      "|         101|             ALBANIA|\n",
      "|         316|             ALGERIA|\n",
      "+------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse SAS_labels_descriptions file for valid country codes\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DecimalType\n",
    "f = open(\"data/I94_SAS_Labels_Descriptions.SAS\",\"r\")\n",
    "lines = f.readlines()\n",
    "countries=[]\n",
    "for line in lines[9:245]:\n",
    "    line_split = line.replace(\"'\", \"\").split('=')\n",
    "    countries.append({\"country_code\":int(line_split[0].strip()), \"country_name\":line_split[1].strip()})\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('country_code', IntegerType()),\n",
    "    StructField('country_name', StringType())\n",
    "])\n",
    "\n",
    "df_countries = spark.createDataFrame(countries, schema)\n",
    "df_countries.show(4)\n",
    "df_countries.createOrReplaceTempView(\"countries\")\n",
    "df_countries.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|airport_code|        airport_name|\n",
      "+------------+--------------------+\n",
      "|         ALC|           ALCAN, AK|\n",
      "|         ANC|       ANCHORAGE, AK|\n",
      "|         BAR|BAKER AAF - BAKER...|\n",
      "|         DAC|   DALTONS CACHE, AK|\n",
      "+------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "591"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse SAS_labels_descriptions file for valid airport code\n",
    "f = open(\"data/I94_SAS_Labels_Descriptions.SAS\", \"r\")\n",
    "lines = f.readlines()\n",
    "airports=[]\n",
    "for line in lines[302:893]:\n",
    "    line_split = line.replace(\"'\", \"\").split('=')\n",
    "    airports.append({\"airport_code\":line_split[0].strip(), \"airport_name\":line_split[1].strip()})\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('airport_code', StringType()),\n",
    "    StructField('airport_name', StringType())\n",
    "])\n",
    "\n",
    "df_airport_codes = spark.createDataFrame(airports, schema)\n",
    "df_airport_codes.show(4)\n",
    "df_airport_codes.createOrReplaceTempView(\"lookup_airport_codes\")\n",
    "df_airport_codes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394068"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_visits.createOrReplaceTempView(\"visits\")\n",
    "\n",
    "spark.sql(\"\"\"select cicid,i94cit,i94res,i94port from visits \n",
    "where i94port not in (select airport_code from lookup_airport_codes)\"\"\").count()\n",
    "\n",
    "spark.sql(\"\"\"select cicid,i94cit,i94res,i94port from visits \n",
    "where i94cit not in (select country_code from countries)\"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----------------+----+------------+--------------+-----+---+------+---------+-------+\n",
      "|     id|country_citizenship|country_residence|port|arrival_date|departure_date|state|age|gender|visa_type|airline|\n",
      "+-------+-------------------+-----------------+----+------------+--------------+-----+---+------+---------+-------+\n",
      "|4557229|                392|              392| FMY|  2016-04-24|    2016-04-29|   NY| 48|     M|        2|     AF|\n",
      "|3908787|                392|              392| FMY|  2016-04-21|          null|   PA|  3|  null|        2|     AT|\n",
      "|5515827|                392|              392| DET|  2016-04-29|    2016-05-05|   PA| 64|     M|        1|     DL|\n",
      "+-------+-------------------+-----------------+----+------------+--------------+-----+---+------+---------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename columns(meaningful), fix column data types, convert arr/dep date to datetime, remove invalid airports,countries\n",
    "df_visits.createOrReplaceTempView(\"visits\")\n",
    "\n",
    "df_visits_format = spark.sql(\"\"\"SELECT cast(cicid as int) id, cast(i94cit as int) country_citizenship, \n",
    "cast(i94res as int) country_residence, i94port port, date_add('1960-01-01', cast(arrdate as int)) arrival_date, \n",
    "date_add('1960-01-01', cast(depdate as int)) departure_date, I94addr state, cast(i94bir as int) age,gender, \n",
    "cast(i94visa as int) visa, airline FROM visits \n",
    "\"\"\")\n",
    "\n",
    "df_visits_format.createOrReplaceTempView(\"visits\")\n",
    "\n",
    "df_visits_clean = spark.sql(\"\"\"SELECT id, country_citizenship, country_residence, port, arrival_date, \n",
    "departure_date, state, age,gender, visa,airline FROM visits \n",
    "where port in (select airport_code from lookup_airport_codes)\n",
    "and country_residence in (select country_code from countries)\n",
    "and country_citizenship in (select country_code from countries)\n",
    "\"\"\")\n",
    "\n",
    "df_visits_clean.createOrReplaceTempView(\"visits\")\n",
    "df_visits_clean.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2698166"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_visits_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+----------+------------+---------+----------+\n",
      "|                name|iso_country|iso_region|municipality|iata_code|local_code|\n",
      "+--------------------+-----------+----------+------------+---------+----------+\n",
      "|   Total Rf Heliport|         US|     US-PA|    Bensalem|     null|       00A|\n",
      "|Aero B Ranch Airport|         US|     US-KS|       Leoti|     null|      00AA|\n",
      "|        Lowell Field|         US|     US-AK|Anchor Point|     null|      00AK|\n",
      "+--------------------+-----------+----------+------------+---------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21236"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Discard non-US airports\n",
    "df_airports.createOrReplaceTempView(\"airports\")\n",
    "\n",
    "df_airports_clean = spark.sql(\"\"\"select name, iso_country, iso_region, municipality, iata_code, local_code From airports \n",
    "where iso_country='US' and local_code is not null\n",
    "\"\"\")\n",
    "\n",
    "df_airports_clean.createOrReplaceTempView(\"airports\")\n",
    "\n",
    "df_airports_clean.show(3)\n",
    "df_airports_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+----------+---------------+-----------------+----------------+------------+\n",
      "|       city|state|median_age|male_population|female_population|total_population|foreign_born|\n",
      "+-----------+-----+----------+---------------+-----------------+----------------+------------+\n",
      "|  Harlingen|   TX|      30.1|          32404|            33365|           65769|       10391|\n",
      "|Brownsville|   TX|      30.6|          87689|            96199|          183888|       53301|\n",
      "|    Sunrise|   FL|      37.7|          41471|            51235|           92706|       38209|\n",
      "|  Vacaville|   CA|      35.8|          50091|            46703|           96794|       10577|\n",
      "+-----------+-----+----------+---------------+-----------------+----------------+------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "+--------------------+-----+----------+---------------+-----------------+----------------+------------+\n",
      "|                city|state|median_age|male_population|female_population|total_population|foreign_born|\n",
      "+--------------------+-----+----------+---------------+-----------------+----------------+------------+\n",
      "|             Sunrise|   FL|      37.7|          41471|            51235|           92706|       38209|\n",
      "|           Vacaville|   CA|      35.8|          50091|            46703|           96794|       10577|\n",
      "|North Richland Hills|   TX|      39.9|          35257|            33948|           69205|        7854|\n",
      "|               Hemet|   CA|      36.8|          38622|            45251|           83873|       13330|\n",
      "|       Sandy Springs|   GA|      35.4|          48570|            56777|          105347|       19837|\n",
      "|       Coral Springs|   FL|      37.2|          63316|            66186|          129502|       38552|\n",
      "|               Salem|   OR|      35.4|          80839|            83704|          164543|       19668|\n",
      "|            Rockford|   IL|      36.3|          71076|            78270|          149346|       18323|\n",
      "|            Lakewood|   CA|      39.9|          41523|            40069|           81592|       18274|\n",
      "|               Tampa|   FL|      35.3|         175517|           193511|          369028|       58795|\n",
      "|            MayagÃ¼ez|   PR|      38.1|          30799|            35782|           66581|        null|\n",
      "|        San Clemente|   CA|      45.2|          34076|            31456|           65532|        8109|\n",
      "|          Santa Rosa|   CA|      37.9|          86840|            88149|          174989|       33015|\n",
      "|         Glen Burnie|   MD|      36.5|          33398|            36461|           69859|        6971|\n",
      "|          Fort Myers|   FL|      37.3|          36850|            37165|           74015|       15365|\n",
      "|          Chesapeake|   VA|      36.7|         114964|           120465|          235429|       13966|\n",
      "|       Laguna Niguel|   CA|      45.8|          30193|            35619|           65812|       12571|\n",
      "|         Centreville|   VA|      36.0|          34749|            37033|           71782|       27797|\n",
      "|                Cary|   NC|      39.9|          78932|            81582|          160514|       30008|\n",
      "|               Parma|   OH|      40.8|          38425|            41518|           79943|        6909|\n",
      "+--------------------+-----+----------+---------------+-----------------+----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demo.createOrReplaceTempView(\"demo\")\n",
    "#df_demo.describe()\n",
    "\n",
    "# spark.sql(\"SELECT * FROM demo where city='Quincy' and state='Massachusetts'\").show()\n",
    "\n",
    "df_demo_format = spark.sql(\"\"\"select distinct city,`State Code` state,cast(`Median Age` as float) median_age, \n",
    "cast(`Male Population` as int) male_population,cast(`Female Population` as int) female_population, \n",
    "cast(`Total Population` as int) total_population, cast(`Foreign-born` as int) foreign_born from demo where state is not null\n",
    "\"\"\")\n",
    "\n",
    "df_demo_format.show(4)\n",
    "df_demo_format.count()\n",
    "df_demo_format.describe()\n",
    "\n",
    "df_demo_format.createOrReplaceTempView(\"demo\")\n",
    "spark.sql(\"select * from demo where median_age>35 \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|code|    name|\n",
      "+----+--------+\n",
      "|  AL| ALABAMA|\n",
      "|  AK|  ALASKA|\n",
      "|  AZ| ARIZONA|\n",
      "|  AR|ARKANSAS|\n",
      "+----+--------+\n",
      "only showing top 4 rows\n",
      "\n",
      "First state: {'code': 'AL', 'name': 'ALABAMA'}, Last country: {'code': 'WY', 'name': 'WYOMING'}\n"
     ]
    }
   ],
   "source": [
    "# parse SAS_labels_descriptions file for state codes and names\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DecimalType\n",
    "f = open(\"data/I94_SAS_Labels_Descriptions.SAS\",\"r\")\n",
    "lines = f.readlines()\n",
    "states=[]\n",
    "for line in lines[981:1035]:\n",
    "    line_split = line.replace(\"'\", \"\").split('=')\n",
    "    states.append({\"code\":line_split[0].strip(), \"name\":line_split[1].strip()})\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('code', StringType()),\n",
    "    StructField('name', StringType())\n",
    "])\n",
    "\n",
    "df_states = spark.createDataFrame(states, schema)\n",
    "df_states.show(4)\n",
    "df_states.count()\n",
    "print(f\"First state: {states[0]}, Last state: {states[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|    visa|\n",
      "+---+--------+\n",
      "|  1|Business|\n",
      "|  2|Pleasure|\n",
      "|  3| Student|\n",
      "+---+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse SAS_labels_descriptions file for visa types\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DecimalType\n",
    "f = open(\"data/I94_SAS_Labels_Descriptions.SAS\",\"r\")\n",
    "lines = f.readlines()\n",
    "visa=[]\n",
    "for line in lines[1046:1049]:\n",
    "    line_split = line.replace(\"'\", \"\").split('=')\n",
    "    visa.append({\"id\":int(line_split[0].strip()), \"visa\":line_split[1].strip()})\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', IntegerType()),\n",
    "    StructField('visa', StringType())\n",
    "])\n",
    "\n",
    "df_visa = spark.createDataFrame(visa, schema)\n",
    "df_visa.show(4)\n",
    "df_visa.createOrReplaceTempView(\"visa\")\n",
    "df_visa.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temperature.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temperature.createOrReplaceTempView(\"temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"select * from temperature where dt> to_date('20000101','YYYYMMDD')\").show(4)\n",
    "spark.sql(\"select * from temperature where dt>= '2000-01-01' order by dt\").show(4)\n",
    "#spark.sql(\"select dt,count(*) from temperature group by dt order by dt desc\").show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=\"results/\"\n",
    "df_demo_format.write.mode('overwrite').parquet(results +  \"dim_us_demo.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visits_clean.createOrReplaceTempView(\"visits\")\n",
    "#spark.sql(\"\"\"select visits.*,'tmp'as temp from visits\"\"\").show(3)\n",
    "df_visits_partition = spark.sql(\"\"\"select visits.*,\n",
    "extract(day from arrival_date) arrival_day,\n",
    "extract(month from arrival_date) arrival_month,\n",
    "extract(year from arrival_date) arrival_year from visits\n",
    "\"\"\")\n",
    "\n",
    "df_visits_partition.write.partitionBy(\"arrival_year\",\"arrival_month\",\"arrival_day\").mode('overwrite').parquet(results + \"fact_visits.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----------------+----+------------+--------------+-----+---+------+---------+-------+-----------+-------------+------------+\n",
      "|     id|country_citizenship|country_residence|port|arrival_date|departure_date|state|age|gender|visa_type|airline|arrival_day|arrival_month|arrival_year|\n",
      "+-------+-------------------+-----------------+----+------------+--------------+-----+---+------+---------+-------+-----------+-------------+------------+\n",
      "|4557229|                392|              392| FMY|  2016-04-24|    2016-04-29|   NY| 48|     M|        2|     AF|         24|            4|        2016|\n",
      "|3908787|                392|              392| FMY|  2016-04-21|          null|   PA|  3|  null|        2|     AT|         21|            4|        2016|\n",
      "|5515827|                392|              392| DET|  2016-04-29|    2016-05-05|   PA| 64|     M|        1|     DL|         29|            4|        2016|\n",
      "+-------+-------------------+-----------------+----+------------+--------------+-----+---+------+---------+-------+-----------+-------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_visits_partition.show(3)\n",
    "#spark.sql(\"\"\"select count(*) from visits where arrival_date=to_date('04/07/2016','MM/dd/yyyy')\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "+---+\n",
      "\n",
      "+-------+-------------------+-----------------+----+------------+--------------+-----+---+------+---------+-------+\n",
      "|     id|country_citizenship|country_residence|port|arrival_date|departure_date|state|age|gender|visa_type|airline|\n",
      "+-------+-------------------+-----------------+----+------------+--------------+-----+---+------+---------+-------+\n",
      "|4557229|                392|              392| FMY|  2016-04-24|    2016-04-29|   NY| 48|     M|        2|     AF|\n",
      "|3908787|                392|              392| FMY|  2016-04-21|          null|   PA|  3|  null|        2|     AT|\n",
      "|5515827|                392|              392| DET|  2016-04-29|    2016-05-05|   PA| 64|     M|        1|     DL|\n",
      "|2377315|                392|              392| NOG|  2016-04-13|          null|   AZ| 22|     F|        3|   null|\n",
      "+-------+-------------------+-----------------+----+------------+--------------+-----+---+------+---------+-------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "df_visits_clean.createOrReplaceTempView(\"visits\")\n",
    "df_countries.createOrReplaceTempView(\"countries\")\n",
    "#select count(*) from visits\n",
    "#spark.sql(\"select count(*) from visits where id is null\").show()\n",
    "#spark.sql(\"select id from visits group by id having count(*)>1\").show()\n",
    "#spark.sql(\"select count(*) from visits where port not in (select airport_code from lookup_airport_codes)\").show()\n",
    "#spark.sql(\"select count(*) from visits where country_citizenship not in (select country_code from countries)\").show()\n",
    "#spark.sql(\"select count(*) from visits where country_residence not in (select country_code from countries)\").show()\n",
    "\n",
    "df_visits_clean.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n",
      "+----+-----+\n",
      "|city|state|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demo_format.createOrReplaceTempView(\"demo\")\n",
    "df_states.createOrReplaceTempView(\"states\")\n",
    "\n",
    "df_demo.count()\n",
    "spark.sql(\"\"\"select count(*) from demo where state is  null or city is null\"\"\").show()\n",
    "spark.sql(\"\"\"select city,state from demo group by city,state having count(*)>1\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|local_code|\n",
      "+----------+\n",
      "|      LL69|\n",
      "|      1TS9|\n",
      "|      3MI4|\n",
      "|      K0Q5|\n",
      "|       FHB|\n",
      "|      XS29|\n",
      "|       6X8|\n",
      "|      TA29|\n",
      "|       L11|\n",
      "|      29NC|\n",
      "|      86KS|\n",
      "|      FL35|\n",
      "|      3PA2|\n",
      "|       EDC|\n",
      "|      7MN5|\n",
      "|      IN58|\n",
      "|      07CT|\n",
      "|      6OR8|\n",
      "|      LA75|\n",
      "|      OH91|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airports_clean.createOrReplaceTempView(\"airports\")\n",
    "spark.sql(\"select count(*) from airports where local_code is null\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
