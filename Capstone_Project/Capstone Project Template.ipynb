{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import findspark\n",
    "findspark.init('C:\\spark')\n",
    "               \n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DecimalType\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "In this open-ended project we build a Data Warehouse for the I94 Immigration Data coming from the US National Tourism and Trade Office, and further enrich it with various other datasets such as US City Demographic Data, Airports data, Country and States date, and Visa information.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "* **I94 Immigration Data**: This data comes from https://travel.trade.gov/research/reports/i94/historical/2016.html. There's a sample file so you can take a look at the data in csv format before reading it all in.\n",
    "\n",
    "* **U.S. City Demographic Data**: This data comes from OpenSoft. You can read more about it at https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export\n",
    "\n",
    "* **Airports Data**: This is a simple table of airport codes and corresponding cities. It comes from https://datahub.io/core/airport-codes#data\n",
    "\n",
    "* **Others**: Some other datasets such as Country & State codes, and Visa information have been consolidated in an SAS file included here in this project: data/I94_SAS_Labels_Descriptions.SAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "#df_spark.write.parquet(\"immigration_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data\n",
    "df_visits=spark.read.parquet(\"data/immigration_data\")\n",
    "df_demo = spark.read.csv(\"data/us-cities-demographics.csv\", sep=\";\", header=True)\n",
    "df_airports = spark.read.csv(\"data/airport-codes_csv.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "|5748518.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1|\n",
      "|5748519.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20582.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_visits.show(4)\n",
    "df_visits.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, cicid: string, i94yr: string, i94mon: string, i94cit: string, i94res: string, i94port: string, arrdate: string, i94mode: string, i94addr: string, depdate: string, i94bir: string, i94visa: string, count: string, dtadfile: string, visapost: string, occup: string, entdepa: string, entdepd: string, entdepu: string, matflag: string, biryear: string, dtaddto: string, gender: string, insnum: string, airline: string, admnum: string, fltno: string, visatype: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_visits.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55075"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airports.show(4)\n",
    "df_airports.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, ident: string, type: string, name: string, elevation_ft: string, continent: string, iso_country: string, iso_region: string, municipality: string, gps_code: string, iata_code: string, local_code: string, coordinates: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airports.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|   California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo.show(4)\n",
    "df_demo.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, City: string, State: string, Median Age: string, Male Population: string, Female Population: string, Total Population: string, Number of Veterans: string, Foreign-born: string, Average Household Size: string, State Code: string, Race: string, Count: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First airport: {'airport_code': 'ALC', 'airport_name': 'ALCAN, AK'}, Last airport: {'airport_code': 'YUM', 'airport_name': 'YUMA, AZ'}\n",
      "+------------+--------------------+\n",
      "|airport_code|        airport_name|\n",
      "+------------+--------------------+\n",
      "|         ALC|           ALCAN, AK|\n",
      "|         ANC|       ANCHORAGE, AK|\n",
      "|         BAR|BAKER AAF - BAKER...|\n",
      "|         DAC|   DALTONS CACHE, AK|\n",
      "+------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "591"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse SAS_labels_descriptions file for valid airport codes and read into a DataFrame\n",
    "f = open(\"data/I94_SAS_Labels_Descriptions.SAS\", \"r\")\n",
    "lines = f.readlines()\n",
    "airports=[]\n",
    "for line in lines[302:893]:\n",
    "    line_split = line.replace(\"'\", \"\").split('=')\n",
    "    airports.append({\"airport_code\":line_split[0].strip(), \"airport_name\":line_split[1].strip()})\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('airport_code', StringType()),\n",
    "    StructField('airport_name', StringType())\n",
    "])\n",
    "\n",
    "print(f\"First airport: {airports[0]}, Last airport: {airports[-1]}\")\n",
    "\n",
    "df_airport_codes = spark.createDataFrame(airports, schema)\n",
    "df_airport_codes.show(4)\n",
    "df_airport_codes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First country: {'country_code': 582, 'country_name': 'MEXICO Air Sea, and Not Reported (I-94, no land arrivals)'}, Last country: {'country_code': 315, 'country_name': 'ZIMBABWE'}\n",
      "+------------+--------------------+\n",
      "|country_code|        country_name|\n",
      "+------------+--------------------+\n",
      "|         582|MEXICO Air Sea, a...|\n",
      "|         236|         AFGHANISTAN|\n",
      "|         101|             ALBANIA|\n",
      "|         316|             ALGERIA|\n",
      "+------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse SAS_labels_descriptions file for valid country codes and read into a DataFrame\n",
    "f = open(\"data/I94_SAS_Labels_Descriptions.SAS\",\"r\")\n",
    "lines = f.readlines()\n",
    "countries=[]\n",
    "for line in lines[9:245]:\n",
    "    line_split = line.replace(\"'\", \"\").split('=')\n",
    "    countries.append({\"country_code\":int(line_split[0].strip()), \"country_name\":line_split[1].strip()})\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('country_code', IntegerType()),\n",
    "    StructField('country_name', StringType())\n",
    "])\n",
    "\n",
    "print(f\"First country: {countries[0]}, Last country: {countries[-1]}\")\n",
    "\n",
    "df_countries = spark.createDataFrame(countries, schema)\n",
    "df_countries.show(4)\n",
    "df_countries.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First state: {'code': 'AL', 'name': 'ALABAMA'}, Last state: {'code': 'WY', 'name': 'WYOMING'}\n",
      "+----+--------+\n",
      "|code|    name|\n",
      "+----+--------+\n",
      "|  AL| ALABAMA|\n",
      "|  AK|  ALASKA|\n",
      "|  AZ| ARIZONA|\n",
      "|  AR|ARKANSAS|\n",
      "+----+--------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse SAS_labels_descriptions file for state codes and names and read into a DataFrame\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DecimalType\n",
    "f = open(\"data/I94_SAS_Labels_Descriptions.SAS\",\"r\")\n",
    "lines = f.readlines()\n",
    "states=[]\n",
    "for line in lines[981:1035]:\n",
    "    line_split = line.replace(\"'\", \"\").split('=')\n",
    "    states.append({\"code\":line_split[0].strip(), \"name\":line_split[1].strip()})\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('code', StringType()),\n",
    "    StructField('name', StringType())\n",
    "])\n",
    "\n",
    "print(f\"First state: {states[0]}, Last state: {states[-1]}\")\n",
    "\n",
    "df_states = spark.createDataFrame(states, schema)\n",
    "df_states.show(4)\n",
    "df_states.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First visa: {'id': 1, 'visa': 'Business'}, Last visa: {'id': 3, 'visa': 'Student'}\n",
      "+---+--------+\n",
      "| id|    visa|\n",
      "+---+--------+\n",
      "|  1|Business|\n",
      "|  2|Pleasure|\n",
      "|  3| Student|\n",
      "+---+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse SAS_labels_descriptions file for visa types and read into a DataFrame\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DecimalType\n",
    "f = open(\"data/I94_SAS_Labels_Descriptions.SAS\",\"r\")\n",
    "lines = f.readlines()\n",
    "visa=[]\n",
    "for line in lines[1046:1049]:\n",
    "    line_split = line.replace(\"'\", \"\").split('=')\n",
    "    visa.append({\"id\":int(line_split[0].strip()), \"visa\":line_split[1].strip()})\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', IntegerType()),\n",
    "    StructField('visa', StringType())\n",
    "])\n",
    "\n",
    "print(f\"First visa: {visa[0]}, Last visa: {visa[-1]}\")\n",
    "\n",
    "df_visa = spark.createDataFrame(visa, schema)\n",
    "df_visa.show(4)\n",
    "df_visa.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Steps\n",
    "**Clean Immigration Data**\n",
    "* Pick only relevant columns, rename columns to meaningful names\n",
    "* Fix data types, date formats\n",
    "* Discard invalid airport/country/state/visa codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----------------+----+------------+--------------+-----+---+------+----+-------+\n",
      "|     id|country_citizenship|country_residence|port|arrival_date|departure_date|state|age|gender|visa|airline|\n",
      "+-------+-------------------+-----------------+----+------------+--------------+-----+---+------+----+-------+\n",
      "|2959311|                516|              516| FTL|  2016-04-16|    2016-04-22|   AZ| 41|     M|   1|     BW|\n",
      "|4563478|                516|              516| HOU|  2016-04-24|    2016-04-27|   AZ| 32|     M|   1|     UA|\n",
      "|4563479|                516|              516| HOU|  2016-04-24|    2016-05-01|   AZ| 38|     M|   1|     UA|\n",
      "|1849132|                251|              251| DET|  2016-04-10|    2016-04-15|   AZ| 40|     M|   1|     UA|\n",
      "+-------+-------------------+-----------------+----+------------+--------------+-----+---+------+----+-------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2550725"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_visits.createOrReplaceTempView(\"visits\")\n",
    "df_airport_codes.createOrReplaceTempView(\"airport_codes\")\n",
    "df_countries.createOrReplaceTempView(\"countries\")\n",
    "df_states.createOrReplaceTempView(\"states\")\n",
    "df_visa.createOrReplaceTempView(\"visa\")\n",
    "\n",
    "df_visits_format = spark.sql(\"\"\"SELECT cast(cicid as int) id, cast(i94cit as int) country_citizenship, \n",
    "cast(i94res as int) country_residence, i94port port, date_add('1960-01-01', cast(arrdate as int)) arrival_date, \n",
    "date_add('1960-01-01', cast(depdate as int)) departure_date, I94addr state, cast(i94bir as int) age,gender, \n",
    "cast(i94visa as int) visa, airline FROM visits \n",
    "\"\"\")\n",
    "\n",
    "df_visits_format.createOrReplaceTempView(\"visits\")\n",
    "\n",
    "df_visits_clean = spark.sql(\"\"\"SELECT id, country_citizenship, country_residence, port, arrival_date, \n",
    "departure_date, state, age,gender, visa,airline FROM visits \n",
    "where port in (select airport_code from airport_codes)\n",
    "and country_residence in (select country_code from countries)\n",
    "and country_citizenship in (select country_code from countries)\n",
    "and state in (select code from states)\n",
    "and visa in (select id from visa)\n",
    "\"\"\")\n",
    "\n",
    "#df_visits_clean.createOrReplaceTempView(\"visits\")\n",
    "df_visits_clean.show(4)\n",
    "df_visits_clean.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean Airport Data**\n",
    "* Pick only relevant columns\n",
    "* Discard non-US airports and rows with missing local_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+----------+------------+---------+----------+\n",
      "|                name|iso_country|iso_region|municipality|iata_code|local_code|\n",
      "+--------------------+-----------+----------+------------+---------+----------+\n",
      "|   Total Rf Heliport|         US|     US-PA|    Bensalem|     null|       00A|\n",
      "|Aero B Ranch Airport|         US|     US-KS|       Leoti|     null|      00AA|\n",
      "|        Lowell Field|         US|     US-AK|Anchor Point|     null|      00AK|\n",
      "|        Epps Airpark|         US|     US-AL|     Harvest|     null|      00AL|\n",
      "+--------------------+-----------+----------+------------+---------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21236"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airports.createOrReplaceTempView(\"airports\")\n",
    "\n",
    "df_airports_clean = spark.sql(\"\"\"\n",
    "select name, iso_country, iso_region, municipality, iata_code, local_code\n",
    "from airports where iso_country='US' and local_code is not null\n",
    "\"\"\")\n",
    "\n",
    "#df_airports_clean.createOrReplaceTempView(\"airports\")\n",
    "df_airports_clean.show(4)\n",
    "df_airports_clean.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean Demographics Data**\n",
    "* Pick only relevant columns, rename columns, remove spaces in column names, fix column data types\n",
    "* Discard rows with missing city or state value\n",
    "* Remove duplicates (by City and State) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+----------+---------------+-----------------+----------------+------------+\n",
      "|       city|state|median_age|male_population|female_population|total_population|foreign_born|\n",
      "+-----------+-----+----------+---------------+-----------------+----------------+------------+\n",
      "|  Harlingen|   TX|      30.1|          32404|            33365|           65769|       10391|\n",
      "|Brownsville|   TX|      30.6|          87689|            96199|          183888|       53301|\n",
      "|    Sunrise|   FL|      37.7|          41471|            51235|           92706|       38209|\n",
      "|  Vacaville|   CA|      35.8|          50091|            46703|           96794|       10577|\n",
      "+-----------+-----+----------+---------------+-----------------+----------------+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo.createOrReplaceTempView(\"demo\")\n",
    "\n",
    "df_demo_clean = spark.sql(\"\"\"select distinct city,`State Code` state,cast(`Median Age` as float) median_age, \n",
    "cast(`Male Population` as int) male_population,cast(`Female Population` as int) female_population, \n",
    "cast(`Total Population` as int) total_population, cast(`Foreign-born` as int) foreign_born from demo \n",
    "where state is not null and city is not null\n",
    "\"\"\")\n",
    "\n",
    "df_demo_clean.show(4)\n",
    "df_demo_clean.count()\n",
    "\n",
    "#df_demo_format.createOrReplaceTempView(\"demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
